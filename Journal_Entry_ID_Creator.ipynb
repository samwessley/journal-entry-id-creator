{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Journal Entry ID Creator\n",
        "\n",
        "**Automatically create balanced journal entries from your Excel data in 3 simple steps!**\n",
        "\n",
        "This tool will:\n",
        "- ‚úÖ Create balanced journal entries (debits = credits)\n",
        "- ‚úÖ Group lines by date and matching fields\n",
        "- ‚úÖ Assign Journal IDs to every line\n",
        "- ‚úÖ Handle minimum 2-line requirement\n",
        "- ‚úÖ Download results instantly\n",
        "\n",
        "## üöÄ Instructions:\n",
        "1. **Run the setup** (click ‚ñ∂Ô∏è on the cell below)\n",
        "2. **Upload your Excel file** when prompted\n",
        "3. **Download your results** with Journal IDs added\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üîß **STEP 1: Setup & Configuration** { display-mode: \"form\" }\n",
        "# @markdown Click the ‚ñ∂Ô∏è button to install required packages and load the journal entry creator.\n",
        "\n",
        "print(\"üîß Setting up Journal Entry ID Creator...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "\n",
        "try:\n",
        "    install_package(\"pandas\")\n",
        "    install_package(\"openpyxl\")\n",
        "    print(\"‚úÖ Required packages installed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error installing packages: {e}\")\n",
        "\n",
        "# Import libraries\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from itertools import combinations\n",
        "    from decimal import Decimal, ROUND_HALF_UP\n",
        "    import io\n",
        "    from pathlib import Path\n",
        "    from google.colab import files\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    print(\"‚úÖ Libraries imported successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error importing libraries: {e}\")\n",
        "\n",
        "# Load the Journal Entry Creator\n",
        "class JournalEntryCreator:\n",
        "    def __init__(self):\n",
        "        self.journal_lines = None\n",
        "        self.grouped_entries = {}\n",
        "        self.unassigned_lines = []\n",
        "        self.additional_output_lines = []  # rows to append to output (e.g., plug lines)\n",
        "        self._id_counts = {}\n",
        "        self.workbook = None\n",
        "        self.ws_jel = None\n",
        "        self.ws_ctb = None\n",
        "        self.ws_type_row = None\n",
        "        self.ws_header_row = None\n",
        "        self.ws_data_start_row = None\n",
        "        self.template_type = 'old'\n",
        "        self.skip_id_creation = False\n",
        "        self._plug_lines_added = False\n",
        "\n",
        "    def _to_decimal(self, value):\n",
        "        try:\n",
        "            return Decimal(str(value))\n",
        "        except Exception:\n",
        "            return Decimal('0')\n",
        "\n",
        "    def _sum_decimal(self, series):\n",
        "        total = Decimal('0')\n",
        "        for v in series.fillna(0).tolist():\n",
        "            total += self._to_decimal(v)\n",
        "        return total.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)\n",
        "\n",
        "    def _sanitize_field_name(self, name):\n",
        "        s = str(name).strip()\n",
        "        return ''.join(ch if ch.isalnum() or ch in (' ', '_', '-') else '_' for ch in s).replace(' ', '_')\n",
        "\n",
        "    def _format_value_token(self, value):\n",
        "        if pd.isna(value):\n",
        "            return 'NA'\n",
        "        if isinstance(value, (pd.Timestamp, np.datetime64)):\n",
        "            try:\n",
        "                dt = pd.to_datetime(value)\n",
        "                return dt.strftime('%Y%m%d')\n",
        "            except Exception:\n",
        "                return self._sanitize_field_name(str(value))\n",
        "        if isinstance(value, (float, int, np.floating, np.integer)):\n",
        "            return str(value).replace('.', '_')\n",
        "        return self._sanitize_field_name(str(value))\n",
        "\n",
        "    def _normalize_header_token(self, token):\n",
        "        if token is None:\n",
        "            return ''\n",
        "        text = str(token)\n",
        "        text = text.replace('\\u00A0', ' ')\n",
        "        text = ' '.join(text.strip().split())\n",
        "        return text\n",
        "\n",
        "    def _canonical(self, token):\n",
        "        return self._normalize_header_token(token).lower()\n",
        "\n",
        "    def _find_header_row(self, df_all, required_names, max_scan_rows=10):\n",
        "        \"\"\"Find a row in df_all (header=None) that contains all required column names (case/space-insensitive).\"\"\"\n",
        "        required = {self._canonical(n) for n in required_names}\n",
        "        rows_to_scan = min(max_scan_rows, len(df_all))\n",
        "        for r in range(rows_to_scan):\n",
        "            row_vals = [self._normalize_header_token(v) for v in df_all.iloc[r].tolist()]\n",
        "            canon = {self._canonical(v) for v in row_vals}\n",
        "            if required.issubset(canon):\n",
        "                # Build unique headers preserving original tokens (normalized for cleanliness)\n",
        "                seen = {}\n",
        "                headers = []\n",
        "                for v in row_vals:\n",
        "                    name = v if v != '' else 'Unnamed'\n",
        "                    count = seen.get(name, 0)\n",
        "                    unique = f\"{name}_{count}\" if count > 0 else name\n",
        "                    seen[name] = count + 1\n",
        "                    headers.append(unique)\n",
        "                return r, headers\n",
        "        return None, None\n",
        "\n",
        "    def generate_journal_id(self, grouping_fields, group_key, max_len=100):\n",
        "        fields = list(grouping_fields) if isinstance(grouping_fields, (list, tuple)) else [grouping_fields]\n",
        "        if isinstance(group_key, tuple):\n",
        "            values = list(group_key)\n",
        "        else:\n",
        "            values = [group_key]\n",
        "        min_len = min(len(fields), len(values))\n",
        "        tokens = [self._format_value_token(values[i]) for i in range(min_len)]\n",
        "        base = 'ID' if not tokens else '-'.join(tokens)\n",
        "        if len(base) > max_len:\n",
        "            base = base[:max_len]\n",
        "        count = self._id_counts.get(base, 0)\n",
        "        self._id_counts[base] = count + 1\n",
        "        if count == 0:\n",
        "            return base\n",
        "        suffix = f\"__{count+1}\"\n",
        "        if len(base) + len(suffix) > max_len:\n",
        "            base = base[: max_len - len(suffix)]\n",
        "        return f\"{base}{suffix}\"\n",
        "        \n",
        "    def _normalize_and_deduplicate_columns(self, columns):\n",
        "        normalized = []\n",
        "        for col in columns:\n",
        "            name = '' if pd.isna(col) else str(col).strip()\n",
        "            if name.lower() in ('', 'nan', 'none'):\n",
        "                name = 'Unnamed'\n",
        "            normalized.append(name)\n",
        "        seen = {}\n",
        "        unique_cols = []\n",
        "        for name in normalized:\n",
        "            count = seen.get(name, 0)\n",
        "            unique_name = f\"{name}_{count}\" if count > 0 else name\n",
        "            seen[name] = count + 1\n",
        "            unique_cols.append(unique_name)\n",
        "        return unique_cols\n",
        "        \n",
        "    def load_data_from_uploaded_file(self, uploaded_file_data, filename):\n",
        "        \"\"\"Load journal lines from uploaded Excel file data\"\"\"\n",
        "        try:\n",
        "            from openpyxl import load_workbook\n",
        "            # Try to load as new template first (Journal Entries & Lines sheet)\n",
        "            try:\n",
        "                wb = load_workbook(io.BytesIO(uploaded_file_data))\n",
        "                if 'Journal Entries & Lines' in wb.sheetnames:\n",
        "                    self.workbook = wb\n",
        "                    self.ws_jel = wb['Journal Entries & Lines']\n",
        "                    self.ws_ctb = wb['Comparative Trial Balances'] if 'Comparative Trial Balances' in wb.sheetnames else None\n",
        "                    self.template_type = 'new'\n",
        "                    # Read raw sheet to detect header row robustly\n",
        "                    df_all = pd.read_excel(io.BytesIO(uploaded_file_data), sheet_name='Journal Entries & Lines', header=None)\n",
        "                    if len(df_all) == 0:\n",
        "                        print(\"‚ùå No data found in Journal Entries & Lines sheet.\")\n",
        "                        return False\n",
        "                    req = ['Posted Date', 'Account ID', 'Debit Amount', 'Credit Amount']\n",
        "                    header_row, headers = self._find_header_row(df_all, req, max_scan_rows=10)\n",
        "                    if header_row is None:\n",
        "                        print(\"‚ùå Required column 'Posted Date' not found in Journal Entries & Lines sheet\")\n",
        "                        return False\n",
        "                    # header_row is 0-based index where actual field names are found\n",
        "                    # In new template: row 0 = Required/Optional, row 1 = field names, row 2+ = data\n",
        "                    # But _find_header_row returns the row with field names (row 1, zero-indexed)\n",
        "                    self.ws_type_row = header_row  # 0-based row with Required/Optional (row 1 in Excel)\n",
        "                    self.ws_header_row = header_row + 1  # Field names row in Excel (1-based)\n",
        "                    self.ws_data_start_row = header_row + 2  # Data starts here in Excel (1-based)\n",
        "                    df = df_all.iloc[header_row+1:].copy()\n",
        "                    df.columns = headers[:len(df.columns)]\n",
        "                    # Normalize critical columns by canonical name lookup\n",
        "                    # Build mapping from canonical -> actual name\n",
        "                    canon_map = {self._canonical(col): col for col in df.columns}\n",
        "                    try:\n",
        "                        posted_col = canon_map[self._canonical('Posted Date')]\n",
        "                        acct_col = canon_map[self._canonical('Account ID')]\n",
        "                        debit_col = canon_map[self._canonical('Debit Amount')]\n",
        "                        credit_col = canon_map[self._canonical('Credit Amount')]\n",
        "                    except KeyError:\n",
        "                        print(\"‚ùå Required columns not found after header normalization\")\n",
        "                        return False\n",
        "                    # Rename to standard names for internal processing\n",
        "                    df = df.rename(columns={posted_col: 'Posted Date', acct_col: 'Account ID', debit_col: 'Debit Amount', credit_col: 'Credit Amount'})\n",
        "                    df = df.dropna(how='all')\n",
        "                    df['Posted Date'] = pd.to_datetime(df['Posted Date'], errors='coerce').dt.normalize()\n",
        "                    df['Debit Amount'] = pd.to_numeric(df['Debit Amount'], errors='coerce').fillna(0)\n",
        "                    df['Credit Amount'] = pd.to_numeric(df['Credit Amount'], errors='coerce').fillna(0)\n",
        "                    df = df.reset_index(drop=True)\n",
        "                    df['_row_index'] = range(len(df))\n",
        "                    self.journal_lines = df\n",
        "                    print(f\"‚úÖ Loaded {len(df)} journal lines from 'Journal Entries & Lines' in {filename}\")\n",
        "                    return True\n",
        "            except Exception:\n",
        "                # Fall back to old template if anything fails\n",
        "                pass\n",
        "\n",
        "            # Old template handling\n",
        "            df_all = pd.read_excel(io.BytesIO(uploaded_file_data), header=None)\n",
        "            \n",
        "            if len(df_all) == 0:\n",
        "                print(\"‚ùå No data found in Excel file.\")\n",
        "                return False\n",
        "            \n",
        "            first_row = df_all.iloc[0].values\n",
        "            second_row = df_all.iloc[1].values if len(df_all) > 1 else None\n",
        "            \n",
        "            if second_row is not None and isinstance(second_row[0], str) and 'Posted Date' in str(second_row[0]):\n",
        "                print(\"‚úÖ Detected field names in second row\")\n",
        "                template_row = second_row\n",
        "                df = df_all.iloc[2:].copy()\n",
        "            elif isinstance(first_row[0], str) and 'Posted Date' in str(first_row[0]):\n",
        "                print(\"‚úÖ Detected template headers in first row\")\n",
        "                template_row = first_row\n",
        "                df = df_all.iloc[1:].copy()\n",
        "            else:\n",
        "                print(\"‚úÖ Using default column structure\")\n",
        "                template_row = ['Posted Date', 'Account ID', 'Debit Amount', 'Credit Amount'] + [f'Optional_{i}' for i in range(len(df_all.columns) - 4)]\n",
        "                df = df_all.copy()\n",
        "            \n",
        "            if len(df) == 0:\n",
        "                print(\"‚ùå No data found. Please add journal line data.\")\n",
        "                return False\n",
        "            \n",
        "            df.columns = self._normalize_and_deduplicate_columns(template_row[:len(df.columns)])\n",
        "            df = df.dropna(how='all')\n",
        "            \n",
        "            required_cols = ['Posted Date', 'Account ID', 'Debit Amount', 'Credit Amount']\n",
        "            for col in required_cols:\n",
        "                if col not in df.columns:\n",
        "                    print(f\"‚ùå Required column '{col}' not found\")\n",
        "                    return False\n",
        "            \n",
        "            df['Posted Date'] = pd.to_datetime(df['Posted Date'], errors='coerce')\n",
        "            df['Posted Date'] = df['Posted Date'].dt.normalize()\n",
        "            df['Debit Amount'] = pd.to_numeric(df['Debit Amount'], errors='coerce').fillna(0)\n",
        "            df['Credit Amount'] = pd.to_numeric(df['Credit Amount'], errors='coerce').fillna(0)\n",
        "            df['_row_index'] = range(len(df))\n",
        "            \n",
        "            self.journal_lines = df\n",
        "            print(f\"üìä Loaded {len(df)} journal lines from {filename}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading Excel file: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def get_optional_fields(self):\n",
        "        if self.journal_lines is None:\n",
        "            return []\n",
        "        \n",
        "        required_cols = ['Posted Date', 'Account ID', 'Debit Amount', 'Credit Amount', '_row_index']\n",
        "        optional_cols = []\n",
        "        \n",
        "        for col in self.journal_lines.columns:\n",
        "            if col not in required_cols:\n",
        "                try:\n",
        "                    non_empty = self.journal_lines[col].notna() & (self.journal_lines[col] != '') & (self.journal_lines[col] != '[INSERT FIELD NAME]')\n",
        "                    if non_empty.any():\n",
        "                        optional_cols.append(col)\n",
        "                except:\n",
        "                    continue\n",
        "        return optional_cols\n",
        "    \n",
        "    def validate_balances(self, epsilon=0.01, return_details=False):\n",
        "        \"\"\"Validate balances overall, by posted date, and by month.\n",
        "\n",
        "        If return_details is True, returns a dict with diagnostics:\n",
        "            {\n",
        "              'ok': bool,\n",
        "              'overall': {'debits': float, 'credits': float, 'net': float},\n",
        "              'by_date': DataFrame with debits, credits, net, diff,\n",
        "              'unbalanced_dates': DataFrame subset,\n",
        "              'by_month': DataFrame with debits, credits, net, diff,\n",
        "              'unbalanced_months': DataFrame subset,\n",
        "              'messages': [str, ...]\n",
        "            }\n",
        "        Otherwise, raises ValueError on failure, returns True on success.\n",
        "        \"\"\"\n",
        "        if self.journal_lines is None or len(self.journal_lines) == 0:\n",
        "            if return_details:\n",
        "                return {'ok': False, 'messages': [\"No journal lines loaded to validate.\"]}\n",
        "            raise ValueError(\"No journal lines loaded to validate.\")\n",
        "        if 'Posted Date' not in self.journal_lines.columns:\n",
        "            if return_details:\n",
        "                return {'ok': False, 'messages': [\"Required column 'Posted Date' is missing.\"]}\n",
        "            raise ValueError(\"Required column 'Posted Date' is missing.\")\n",
        "\n",
        "        df = self.journal_lines.copy()\n",
        "        details = {'messages': []}\n",
        "\n",
        "        # Overall\n",
        "        total_debits = self._sum_decimal(df['Debit Amount'])\n",
        "        total_credits = self._sum_decimal(df['Credit Amount'])\n",
        "        overall_net = (total_debits - total_credits)\n",
        "        details['overall'] = {'debits': total_debits, 'credits': total_credits, 'net': overall_net}\n",
        "        if abs(overall_net) >= epsilon:\n",
        "            details['messages'].append(\n",
        "                f\"Overall debits and credits do not balance. Total Debits: ${total_debits:,.2f}, Total Credits: ${total_credits:,.2f}, Difference: ${overall_net:,.2f}.\"\n",
        "            )\n",
        "\n",
        "        # Per date\n",
        "        df['__date__'] = df['Posted Date'].dt.date\n",
        "        by_date = df.groupby('__date__', dropna=False)[['Debit Amount', 'Credit Amount']].sum()\n",
        "        by_date = by_date.rename(columns={'Debit Amount': 'debits', 'Credit Amount': 'credits'})\n",
        "        by_date['debits'] = by_date['debits'].apply(lambda x: Decimal(str(x))).apply(lambda x: x.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP))\n",
        "        by_date['credits'] = by_date['credits'].apply(lambda x: Decimal(str(x))).apply(lambda x: x.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP))\n",
        "        by_date['net'] = by_date['debits'] - by_date['credits']\n",
        "        by_date['diff'] = by_date['net'].abs()\n",
        "        unbalanced_dates = by_date[by_date['diff'] >= epsilon]\n",
        "        details['by_date'] = by_date\n",
        "        details['unbalanced_dates'] = unbalanced_dates\n",
        "        if len(unbalanced_dates) > 0:\n",
        "            msg_lines = [\"Unbalanced posted dates detected (debits != credits):\"]\n",
        "            for d, row in unbalanced_dates.iterrows():\n",
        "                msg_lines.append(\n",
        "                    f\"  - {d}: Debits=${row['debits']:,.2f}, Credits=${row['credits']:,.2f}, Difference=${row['net']:,.2f}\"\n",
        "                )\n",
        "            details['messages'].append(\"\\n\".join(msg_lines))\n",
        "\n",
        "        # Per month\n",
        "        df['__month__'] = df['Posted Date'].dt.to_period('M')\n",
        "        by_month = df.groupby('__month__', dropna=False)[['Debit Amount', 'Credit Amount']].sum()\n",
        "        by_month = by_month.rename(columns={'Debit Amount': 'debits', 'Credit Amount': 'credits'})\n",
        "        by_month['debits'] = by_month['debits'].apply(lambda x: Decimal(str(x))).apply(lambda x: x.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP))\n",
        "        by_month['credits'] = by_month['credits'].apply(lambda x: Decimal(str(x))).apply(lambda x: x.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP))\n",
        "        by_month['net'] = by_month['debits'] - by_month['credits']\n",
        "        by_month['diff'] = by_month['net'].abs()\n",
        "        unbalanced_months = by_month[by_month['diff'] >= epsilon]\n",
        "        details['by_month'] = by_month\n",
        "        details['unbalanced_months'] = unbalanced_months\n",
        "        if len(unbalanced_months) > 0:\n",
        "            msg_lines = [\"Unbalanced posted months detected (debits != credits):\"]\n",
        "            for m, row in unbalanced_months.iterrows():\n",
        "                msg_lines.append(\n",
        "                    f\"  - {m}: Debits=${row['debits']:,.2f}, Credits=${row['credits']:,.2f}, Difference=${row['net']:,.2f}\"\n",
        "                )\n",
        "            details['messages'].append(\"\\n\".join(msg_lines))\n",
        "\n",
        "        # Cleanup\n",
        "        df.drop(columns=['__date__', '__month__'], inplace=True)\n",
        "\n",
        "        ok = len(details['messages']) == 0\n",
        "        details['ok'] = ok\n",
        "        if return_details:\n",
        "            return details\n",
        "        if not ok:\n",
        "            raise ValueError(\"\\n\\n\".join(details['messages']))\n",
        "        return True\n",
        "    \n",
        "    def check_balance(self, group_df):\n",
        "        if len(group_df) < 2:\n",
        "            return False\n",
        "        total_debits = self._sum_decimal(group_df['Debit Amount'])\n",
        "        total_credits = self._sum_decimal(group_df['Credit Amount'])\n",
        "        return total_debits == total_credits\n",
        "    \n",
        "    def generate_grouping_combinations(self, optional_fields, max_fields=5):\n",
        "        combinations_to_try = []\n",
        "        for r in range(min(len(optional_fields), max_fields), 0, -1):\n",
        "            for combo in combinations(optional_fields, r):\n",
        "                combinations_to_try.append(['Posted Date'] + list(combo))\n",
        "        combinations_to_try.append(['Posted Date'])\n",
        "        return combinations_to_try\n",
        "    \n",
        "    def balance_unassigned_with_plug(self, plug_account_id=\"Audit Sight Clearing\", epsilon=0.01):\n",
        "        \"\"\"Create balancing journal entries per posted date for unassigned lines by adding a plug line.\n",
        "\n",
        "        - Groups unassigned lines by Posted Date (date-only)\n",
        "        - For each date group, creates a new JE containing those lines\n",
        "        - Adds one plug line with Account ID = plug_account_id for the offset amount\n",
        "        - Returns number of dates balanced\n",
        "        \"\"\"\n",
        "        if self.journal_lines is None:\n",
        "            print(\"No data loaded. Cannot balance.\")\n",
        "            return 0\n",
        "        if len(self.unassigned_lines) == 0:\n",
        "            print(\"No unassigned lines to balance.\")\n",
        "            return 0\n",
        "\n",
        "        dates_balanced = 0\n",
        "        next_id_num = 1\n",
        "        if len(self.grouped_entries) > 0:\n",
        "            try:\n",
        "                nums = [int(k[2:]) for k in self.grouped_entries.keys() if str(k).startswith(\"JE\")]\n",
        "                if nums:\n",
        "                    next_id_num = max(nums) + 1\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        df_un = self.unassigned_lines.copy()\n",
        "        df_un['__date__'] = df_un['Posted Date'].dt.date\n",
        "        for date_value, group in df_un.groupby('__date__'):\n",
        "            group_clean = group.copy().reset_index(drop=True)\n",
        "            total_debits = self._sum_decimal(group_clean['Debit Amount'])\n",
        "            total_credits = self._sum_decimal(group_clean['Credit Amount'])\n",
        "            net = total_debits - total_credits\n",
        "\n",
        "            je_id = f\"JE{next_id_num:04d}\"\n",
        "            next_id_num += 1\n",
        "\n",
        "            plug_debit = Decimal('0.00')\n",
        "            plug_credit = Decimal('0.00')\n",
        "            if net != Decimal('0.00'):\n",
        "                if net > 0:\n",
        "                    plug_credit = net.copy_abs().quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)\n",
        "                else:\n",
        "                    plug_debit = net.copy_abs().quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)\n",
        "\n",
        "                plug_row = {col: None for col in self.journal_lines.columns}\n",
        "                plug_row['Posted Date'] = pd.to_datetime(date_value)\n",
        "                plug_row['Account ID'] = plug_account_id\n",
        "                plug_row['Debit Amount'] = float(plug_debit)\n",
        "                plug_row['Credit Amount'] = float(plug_credit)\n",
        "                plug_row['_row_index'] = -1\n",
        "\n",
        "                group_with_plug = pd.concat([group_clean, pd.DataFrame([plug_row])], ignore_index=True)\n",
        "            else:\n",
        "                group_with_plug = group_clean\n",
        "\n",
        "            self.grouped_entries[je_id] = {\n",
        "                'lines': group_with_plug.reset_index(drop=True),\n",
        "                'grouping_fields': ['Posted Date'],\n",
        "                'group_key': str(date_value),\n",
        "                'total_debits': float(group_with_plug['Debit Amount'].sum()),\n",
        "                'total_credits': float(group_with_plug['Credit Amount'].sum())\n",
        "            }\n",
        "\n",
        "            # Record additional output line for plug\n",
        "            if net != Decimal('0.00'):\n",
        "                add_line = {'Journal ID': je_id}\n",
        "                add_line['Posted Date'] = pd.to_datetime(date_value)\n",
        "                add_line['Account ID'] = plug_account_id\n",
        "                add_line['Debit Amount'] = float(plug_debit)\n",
        "                add_line['Credit Amount'] = float(plug_credit)\n",
        "                # Include any extra columns present in data with None defaults\n",
        "                for col in self.journal_lines.columns:\n",
        "                    if col in ('Posted Date', 'Account ID', 'Debit Amount', 'Credit Amount', '_row_index'):\n",
        "                        continue\n",
        "                    add_line[col] = None\n",
        "                self.additional_output_lines.append(add_line)\n",
        "                self._plug_lines_added = True\n",
        "\n",
        "            dates_balanced += 1\n",
        "\n",
        "        self.unassigned_lines = self.journal_lines.iloc[0:0].copy()\n",
        "        return dates_balanced\n",
        "\n",
        "    def add_plug_lines_for_imbalances(self, details, plug_account_id=\"Audit Sight Clearing\", epsilon=0.01):\n",
        "        \"\"\"Append plug lines to fix overall/date/month imbalances before grouping.\n",
        "\n",
        "        Strategy:\n",
        "        - If per-date imbalances exist: add one plug per unbalanced date on that date\n",
        "        - Else if per-month imbalances exist: add one plug per unbalanced month on the last date present in that month\n",
        "        - Else if only overall imbalance exists: add one plug dated on the latest Posted Date in data\n",
        "        Returns number of plugs added.\n",
        "        \"\"\"\n",
        "        if self.journal_lines is None or len(self.journal_lines) == 0:\n",
        "            return 0\n",
        "\n",
        "        df = self.journal_lines\n",
        "        next_row_index = int(df['_row_index'].max()) + 1 if '_row_index' in df.columns and len(df) > 0 else 0\n",
        "        plugs_added = 0\n",
        "\n",
        "        def append_plug_row(date_value, net):\n",
        "            nonlocal next_row_index, plugs_added\n",
        "            if abs(net) < epsilon:\n",
        "                return\n",
        "            plug_debit = 0.0\n",
        "            plug_credit = 0.0\n",
        "            if net > 0:\n",
        "                plug_credit = abs(net)\n",
        "            else:\n",
        "                plug_debit = abs(net)\n",
        "            plug = {\n",
        "                'Posted Date': pd.to_datetime(date_value),\n",
        "                'Account ID': plug_account_id,\n",
        "                'Debit Amount': plug_debit,\n",
        "                'Credit Amount': plug_credit,\n",
        "                '_row_index': next_row_index,\n",
        "            }\n",
        "            for col in df.columns:\n",
        "                if col not in plug:\n",
        "                    plug[col] = None\n",
        "            self.journal_lines = pd.concat([self.journal_lines, pd.DataFrame([plug])], ignore_index=True)\n",
        "            next_row_index += 1\n",
        "            plugs_added += 1\n",
        "            self._plug_lines_added = True\n",
        "            # Also track for output appending\n",
        "            add_line = {col: None for col in self.journal_lines.columns if col != '_row_index'}\n",
        "            add_line['Posted Date'] = pd.to_datetime(date_value)\n",
        "            add_line['Account ID'] = plug_account_id\n",
        "            add_line['Debit Amount'] = plug_debit\n",
        "            add_line['Credit Amount'] = plug_credit\n",
        "            self.additional_output_lines.append(add_line)\n",
        "\n",
        "        # Prefer date-level fix when present\n",
        "        unbalanced_dates = details.get('unbalanced_dates')\n",
        "        if unbalanced_dates is not None and len(unbalanced_dates) > 0:\n",
        "            for date_value, row in unbalanced_dates.iterrows():\n",
        "                append_plug_row(date_value, float(row['net']))\n",
        "            return plugs_added\n",
        "\n",
        "        # Otherwise month-level fix\n",
        "        unbalanced_months = details.get('unbalanced_months')\n",
        "        if unbalanced_months is not None and len(unbalanced_months) > 0:\n",
        "            df_tmp = self.journal_lines.copy()\n",
        "            df_tmp['__month__'] = df_tmp['Posted Date'].dt.to_period('M')\n",
        "            for month_period, row in unbalanced_months.iterrows():\n",
        "                candidates = df_tmp[df_tmp['__month__'] == month_period]['Posted Date']\n",
        "                if len(candidates) > 0:\n",
        "                    date_value = pd.to_datetime(candidates.max())\n",
        "                else:\n",
        "                    date_value = pd.Period(month_period, freq='M').to_timestamp(how='end')\n",
        "                append_plug_row(date_value, float(row['net']))\n",
        "            return plugs_added\n",
        "\n",
        "        # Otherwise overall only\n",
        "        net = details.get('overall', {}).get('net', 0.0)\n",
        "        if abs(net) >= epsilon:\n",
        "            last_date = pd.to_datetime(self.journal_lines['Posted Date'].max())\n",
        "            append_plug_row(last_date, float(net))\n",
        "        return plugs_added\n",
        "\n",
        "    def create_journal_entries(self, max_optional_fields=5):\n",
        "        if self.journal_lines is None:\n",
        "            print(\"‚ùå No data loaded\")\n",
        "            return False\n",
        "        \n",
        "        print(\"üîÑ Creating journal entries...\")\n",
        "        \n",
        "        optional_fields = self.get_optional_fields()\n",
        "        print(f\"üìã Found optional fields: {optional_fields}\")\n",
        "        \n",
        "        field_combinations = self.generate_grouping_combinations(optional_fields, max_optional_fields)\n",
        "        print(f\"üîç Testing {len(field_combinations)} grouping combinations...\")\n",
        "        \n",
        "        assigned_lines = set()\n",
        "        journal_entry_id = 1\n",
        "        \n",
        "        for fields in field_combinations:\n",
        "            # Get unassigned lines and reset index to avoid groupby issues\n",
        "            unassigned_df = self.journal_lines[~self.journal_lines['_row_index'].isin(assigned_lines)].copy().reset_index(drop=True)\n",
        "            \n",
        "            if len(unassigned_df) == 0:\n",
        "                break\n",
        "            \n",
        "            # Dedupe and validate grouping fields\n",
        "            valid_fields = [col for col in fields if col in unassigned_df.columns]\n",
        "            valid_fields = list(dict.fromkeys(valid_fields))\n",
        "            if not valid_fields:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                grouped = unassigned_df.groupby(valid_fields, dropna=False, sort=False)\n",
        "            except Exception as e:\n",
        "                print(f\"   Error grouping by {valid_fields}: {e}\")\n",
        "                continue\n",
        "            \n",
        "            balanced_groups = 0\n",
        "            \n",
        "            for group_key, group_df in grouped:\n",
        "                if self.check_balance(group_df):\n",
        "                    je_id = self.generate_journal_id(valid_fields, group_key)\n",
        "                    \n",
        "                    group_df_clean = group_df.copy().reset_index(drop=True)\n",
        "                    \n",
        "                    self.grouped_entries[je_id] = {\n",
        "                        'lines': group_df_clean,\n",
        "                        'grouping_fields': valid_fields,\n",
        "                        'group_key': group_key,\n",
        "                        'total_debits': group_df_clean['Debit Amount'].sum(),\n",
        "                        'total_credits': group_df_clean['Credit Amount'].sum()\n",
        "                    }\n",
        "                    \n",
        "                    assigned_lines.update(group_df['_row_index'].tolist())\n",
        "                    balanced_groups += 1\n",
        "            \n",
        "            if balanced_groups > 0:\n",
        "                print(f\"   ‚úÖ Created {balanced_groups} entries with grouping: {valid_fields}\")\n",
        "        \n",
        "        # Handle remaining lines\n",
        "        remaining_lines = self.journal_lines[~self.journal_lines['_row_index'].isin(assigned_lines)].copy().reset_index(drop=True)\n",
        "        \n",
        "        if len(remaining_lines) > 0:\n",
        "            print(f\"üìù Processing {len(remaining_lines)} remaining lines...\")\n",
        "            \n",
        "            for _, line in remaining_lines.iterrows():\n",
        "                debit = line['Debit Amount']\n",
        "                credit = line['Credit Amount']\n",
        "                line_date = line['Posted Date']\n",
        "                \n",
        "                if debit == 0 and credit == 0:\n",
        "                    # Try to attach zero-amount line to an existing JE on same date; otherwise leave unassigned\n",
        "                    assigned_to_existing = False\n",
        "                    for je_id, entry_data in self.grouped_entries.items():\n",
        "                        if len(entry_data['lines']) > 0:\n",
        "                            entry_date = entry_data['lines']['Posted Date'].iloc[0]\n",
        "                            if entry_date.date() == line_date.date():\n",
        "                                line_df = pd.DataFrame([line]).reset_index(drop=True)\n",
        "                                existing_lines = entry_data['lines'].copy().reset_index(drop=True)\n",
        "                                entry_data['lines'] = pd.concat([existing_lines, line_df], ignore_index=True)\n",
        "                                entry_data['total_debits'] += line['Debit Amount']\n",
        "                                entry_data['total_credits'] += line['Credit Amount']\n",
        "                                assigned_lines.add(line['_row_index'])\n",
        "                                assigned_to_existing = True\n",
        "                                print(f\"   ‚úÖ Zero-amount line assigned to {je_id}\")\n",
        "                                break\n",
        "                    if not assigned_to_existing:\n",
        "                        # Keep unassigned; will be handled by plug balancing step\n",
        "                        continue\n",
        "                elif debit != 0 and credit != 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    # Do not create single-line entries; leave for plug balancing\n",
        "                    continue\n",
        "        \n",
        "        self.unassigned_lines = self.journal_lines[~self.journal_lines['_row_index'].isin(assigned_lines)].copy()\n",
        "        \n",
        "        print(f\"\\\\nüìä Summary:\")\n",
        "        print(f\"   ‚úÖ Journal entries created: {len(self.grouped_entries)}\")\n",
        "        print(f\"   ‚úÖ Lines assigned: {len(assigned_lines)}\")\n",
        "        print(f\"   ‚ö†Ô∏è  Invalid lines: {len(self.unassigned_lines)}\")\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def generate_output(self, original_filename):\n",
        "        if self.journal_lines is None:\n",
        "            return None\n",
        "        \n",
        "        from openpyxl import load_workbook\n",
        "        \n",
        "        input_path = Path(original_filename)\n",
        "        output_filename = f\"{input_path.stem}_with_journal_ids{input_path.suffix}\"\n",
        "        \n",
        "        # New template: write IDs into workbook and save\n",
        "        if self.template_type == 'new' and self.workbook is not None:\n",
        "            if not self.skip_id_creation:\n",
        "                # Write Journal IDs - ALWAYS write to column A (column 1)\n",
        "                jid_col_idx = 1\n",
        "                # Set header in column A\n",
        "                self.ws_jel.cell(row=self.ws_header_row, column=jid_col_idx, value='Journal ID')\n",
        "                # Build complete header list for plug line mapping\n",
        "                header_cells = self.ws_jel[self.ws_header_row]\n",
        "                headers = [cell.value for cell in header_cells]\n",
        "                \n",
        "                # Find the last row with actual data (not just Journal IDs we might write)\n",
        "                last_data_row = self.ws_data_start_row - 1\n",
        "                for row in self.ws_jel.iter_rows(min_row=self.ws_data_start_row):\n",
        "                    # Check if any cell in this row (excluding column A) has data\n",
        "                    has_data = any(cell.value is not None for cell in row[1:])  # Skip column A\n",
        "                    if has_data:\n",
        "                        last_data_row = row[0].row\n",
        "                    else:\n",
        "                        break\n",
        "                \n",
        "                # Write Journal IDs to column A for all assigned lines (skip plug lines with _row_index < 0)\n",
        "                for je_id, entry_data in self.grouped_entries.items():\n",
        "                    row_indices = entry_data['lines']['_row_index'].tolist()\n",
        "                    for idx in row_indices:\n",
        "                        # Skip plug lines that were added during balancing (they have negative row index)\n",
        "                        if int(idx) < 0:\n",
        "                            continue\n",
        "                        ws_row = self.ws_data_start_row + int(idx)\n",
        "                        self.ws_jel.cell(row=ws_row, column=jid_col_idx, value=je_id)\n",
        "                \n",
        "                # Append plug lines to the bottom of the sheet - start right after last data row\n",
        "                if self.additional_output_lines:\n",
        "                    # Map column names to indices\n",
        "                    col_to_idx = {}\n",
        "                    for i, h in enumerate(headers):\n",
        "                        if h:\n",
        "                            col_to_idx[h] = i + 1\n",
        "                    # Also need to map our internal column names to sheet columns\n",
        "                    # Find Posted Date, Account ID, Debit Amount, Credit Amount columns\n",
        "                    for i, h in enumerate(headers):\n",
        "                        if h and 'posted' in str(h).lower() and 'date' in str(h).lower():\n",
        "                            col_to_idx['Posted Date'] = i + 1\n",
        "                        elif h and 'account' in str(h).lower() and 'id' in str(h).lower():\n",
        "                            col_to_idx['Account ID'] = i + 1\n",
        "                        elif h and 'debit' in str(h).lower():\n",
        "                            col_to_idx['Debit Amount'] = i + 1\n",
        "                        elif h and 'credit' in str(h).lower():\n",
        "                            col_to_idx['Credit Amount'] = i + 1\n",
        "                    \n",
        "                    # Write each plug line row starting from last_data_row + 1\n",
        "                    plug_row_start = last_data_row + 1\n",
        "                    for idx, plug_line in enumerate(self.additional_output_lines):\n",
        "                        new_row_idx = plug_row_start + idx\n",
        "                        # Write Journal ID if present in plug_line dict\n",
        "                        if 'Journal ID' in plug_line:\n",
        "                            self.ws_jel.cell(row=new_row_idx, column=jid_col_idx, value=plug_line['Journal ID'])\n",
        "                        # Write other fields\n",
        "                        for key, val in plug_line.items():\n",
        "                            if key == 'Journal ID':\n",
        "                                continue\n",
        "                            if key in col_to_idx:\n",
        "                                self.ws_jel.cell(row=new_row_idx, column=col_to_idx[key], value=val)\n",
        "                # Add clearing account to CTB if plug lines were added\n",
        "                if self._plug_lines_added and self.ws_ctb is not None:\n",
        "                    ctbr = self.ws_ctb.max_row + 1\n",
        "                    self.ws_ctb.cell(row=ctbr, column=1, value='Audit Sight Clearing')\n",
        "                    self.ws_ctb.cell(row=ctbr, column=2, value='Audit Sight Clearing')\n",
        "                    self.ws_ctb.cell(row=ctbr, column=3, value=0)\n",
        "                    self.ws_ctb.cell(row=ctbr, column=4, value=0)\n",
        "                    self.ws_ctb.cell(row=ctbr, column=5, value='Assets')\n",
        "                    self.ws_ctb.cell(row=ctbr, column=6, value='asset:current:other')\n",
        "                    print(\"‚úÖ Audit Sight Clearing account added to Comparative Trial Balances tab.\")\n",
        "            \n",
        "            output_buffer = io.BytesIO()\n",
        "            self.workbook.save(output_buffer)\n",
        "            output_buffer.seek(0)\n",
        "            self.print_summary_report()\n",
        "            return output_buffer.getvalue(), output_filename\n",
        "        \n",
        "        # Old template: create new DataFrame output\n",
        "        output_df = self.journal_lines.copy()\n",
        "        output_df['Journal ID'] = ''\n",
        "        \n",
        "        for je_id, entry_data in self.grouped_entries.items():\n",
        "            row_indices = entry_data['lines']['_row_index'].tolist()\n",
        "            output_df.loc[output_df['_row_index'].isin(row_indices), 'Journal ID'] = je_id\n",
        "        \n",
        "        output_df = output_df.drop('_row_index', axis=1)\n",
        "        \n",
        "        # Append any additional output lines (plug lines)\n",
        "        if self.additional_output_lines:\n",
        "            cols_set = set(output_df.columns)\n",
        "            add_rows = []\n",
        "            for row in self.additional_output_lines:\n",
        "                completed = {col: row.get(col, None) for col in output_df.columns}\n",
        "                add_rows.append(completed)\n",
        "            if add_rows:\n",
        "                output_df = pd.concat([output_df, pd.DataFrame(add_rows)], ignore_index=True)\n",
        "        \n",
        "        cols = list(output_df.columns)\n",
        "        cols.remove('Journal ID')\n",
        "        posted_date_idx = cols.index('Posted Date')\n",
        "        cols.insert(posted_date_idx, 'Journal ID')\n",
        "        output_df = output_df[cols]\n",
        "        \n",
        "        output_buffer = io.BytesIO()\n",
        "        output_df.to_excel(output_buffer, index=False)\n",
        "        output_buffer.seek(0)\n",
        "        \n",
        "        self.print_summary_report()\n",
        "        \n",
        "        return output_buffer.getvalue(), output_filename\n",
        "    \n",
        "    def print_summary_report(self):\n",
        "        print(\"\\\\n\" + \"=\"*50)\n",
        "        print(\"üìã JOURNAL ENTRY SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        multi_line_entries = []\n",
        "        single_line_entries = []\n",
        "        \n",
        "        for je_id, entry_data in sorted(self.grouped_entries.items()):\n",
        "            if len(entry_data['lines']) > 1:\n",
        "                multi_line_entries.append((je_id, entry_data))\n",
        "            else:\n",
        "                single_line_entries.append((je_id, entry_data))\n",
        "        \n",
        "        if multi_line_entries:\n",
        "            print(f\"\\\\nüîó MULTI-LINE ENTRIES ({len(multi_line_entries)}):\")\n",
        "            for je_id, entry_data in multi_line_entries[:5]:\n",
        "                lines = entry_data['lines']\n",
        "                print(f\"   {je_id}: {lines['Posted Date'].iloc[0].strftime('%Y-%m-%d')} | {len(lines)} lines | ${entry_data['total_debits']:,.2f}\")\n",
        "            \n",
        "            if len(multi_line_entries) > 5:\n",
        "                print(f\"   ... and {len(multi_line_entries) - 5} more\")\n",
        "        \n",
        "        if single_line_entries:\n",
        "            print(f\"\\\\nüìÑ SINGLE-LINE ENTRIES ({len(single_line_entries)}):\")\n",
        "            for je_id, entry_data in single_line_entries[:3]:\n",
        "                lines = entry_data['lines']\n",
        "                line = lines.iloc[0]\n",
        "                print(f\"   {je_id}: {line['Posted Date'].strftime('%Y-%m-%d')} | {line['Account ID']} | ${line['Debit Amount']:.2f}/${line['Credit Amount']:.2f}\")\n",
        "            \n",
        "            if len(single_line_entries) > 3:\n",
        "                print(f\"   ... and {len(single_line_entries) - 3} more\")\n",
        "\n",
        "print(\"‚úÖ Journal Entry Creator loaded successfully!\")\n",
        "print(\"\\nüéØ Ready to process your Excel file!\")\n",
        "print(\"   ‚¨áÔ∏è Run the next cell to upload your file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üìÅ **STEP 2: Upload Your Excel File** { display-mode: \"form\" }\n",
        "# @markdown Click ‚ñ∂Ô∏è to upload your Excel file with journal line data.\n",
        "\n",
        "print(\"üìÅ Upload Your Excel File\")\n",
        "print(\"=\" * 30)\n",
        "print(\"Your file should contain:\")\n",
        "print(\"   ‚úÖ Posted Date column\")\n",
        "print(\"   ‚úÖ Account ID column\") \n",
        "print(\"   ‚úÖ Debit Amount column\")\n",
        "print(\"   ‚úÖ Credit Amount column\")\n",
        "print(\"   üìã Optional fields (Description, Reference, etc.)\")\n",
        "print()\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    file_data = uploaded[filename]\n",
        "    print(f\"\\n‚úÖ Successfully uploaded: {filename}\")\n",
        "    print(f\"   üìä File size: {len(file_data):,} bytes\")\n",
        "    \n",
        "    # Process immediately\n",
        "    print(\"\\nüöÄ Processing your data...\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    creator = JournalEntryCreator()\n",
        "    \n",
        "    if creator.load_data_from_uploaded_file(file_data, filename):\n",
        "        try:\n",
        "            print(\"\\nüßÆ Validating balances (overall, by date, by month if needed)...\")\n",
        "            details = creator.validate_balances(return_details=True)\n",
        "            if not details['ok']:\n",
        "                print(\"‚ùå Balance validation failed:\\n\" + \"\\n\\n\".join(details['messages']))\n",
        "                try:\n",
        "                    resp = input(\"Add plug lines using 'Audit Sight Clearing' to fix these imbalances? [y/N]: \").strip().lower()\n",
        "                except EOFError:\n",
        "                    resp = 'n'\n",
        "                if resp == 'y':\n",
        "                    added = creator.add_plug_lines_for_imbalances(details)\n",
        "                    print(f\"‚úÖ Added {added} plug line(s) to fix imbalances.\")\n",
        "                else:\n",
        "                    raise Exception(\"User declined to auto-balance imbalances.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Balance validation failed:\\n{e}\")\n",
        "        else:\n",
        "            if creator.create_journal_entries():\n",
        "                # If unassigned lines remain, prompt user to auto-balance with plug lines\n",
        "                if len(creator.unassigned_lines) > 0:\n",
        "                    print(f\"\\n‚ö†Ô∏è Found {len(creator.unassigned_lines)} unassigned lines after grouping.\")\n",
        "                    try:\n",
        "                        resp = input(\"Add plug lines using 'Audit Sight Clearing' to balance each affected date? [y/N]: \").strip().lower()\n",
        "                    except EOFError:\n",
        "                        resp = 'n'\n",
        "                    if resp == 'y':\n",
        "                        balanced_dates = creator.balance_unassigned_with_plug()\n",
        "                        print(f\"‚úÖ Added plug lines for {balanced_dates} posted date(s).\")\n",
        "                    else:\n",
        "                        raise Exception(\"User declined to auto-balance unassigned lines.\")\n",
        "                \n",
        "                output_data, output_filename = creator.generate_output(filename)\n",
        "                \n",
        "                print(f\"\\nüéâ SUCCESS! Your file is ready for download.\")\n",
        "                print(f\"   üìÑ Output filename: {output_filename}\")\n",
        "                print(\"\\n‚¨áÔ∏è Run the next cell to download your results!\")\n",
        "            else:\n",
        "                print(\"‚ùå Failed to create journal entries\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to load your Excel file\")\n",
        "else:\n",
        "    print(\"‚ùå No file uploaded. Please run this cell again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title üíæ **STEP 3: Download Your Results** { display-mode: \"form\" }\n",
        "# @markdown Click ‚ñ∂Ô∏è to download your processed Excel file with Journal IDs.\n",
        "\n",
        "if 'output_data' in locals() and 'output_filename' in locals():\n",
        "    print(\"üíæ Downloading Your Results\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    # Save and download the file\n",
        "    with open(output_filename, 'wb') as f:\n",
        "        f.write(output_data)\n",
        "    \n",
        "    files.download(output_filename)\n",
        "    \n",
        "    print(f\"‚úÖ Download started: {output_filename}\")\n",
        "    print(\"\\nüìã Your output file contains:\")\n",
        "    print(\"   ‚úÖ All your original data\")\n",
        "    print(\"   ‚úÖ New 'Journal ID' column\")\n",
        "    print(\"   ‚úÖ Every line has a Journal ID\")\n",
        "    print(\"   ‚úÖ Balanced journal entries\")\n",
        "    print(\"   ‚úÖ Zero-amount lines properly assigned\")\n",
        "    \n",
        "    print(\"\\nüéØ Next Steps:\")\n",
        "    print(\"   üìä Import into your accounting system\")\n",
        "    print(\"   üìà Use Journal IDs for reporting\")\n",
        "    print(\"   üîç Review the summary above\")\n",
        "    \n",
        "    print(\"\\n‚ú® Thank you for using Journal Entry ID Creator!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No processed data available.\")\n",
        "    print(\"   Please run the previous cell to upload and process your file first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
